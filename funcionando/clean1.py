# -*- coding: utf-8 -*-
"""clean1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nxiPvrERVIj3F5_DPEY6to4WbLZdT7Ki

# 1st part
"""

!pip install -q pandas numpy scikit-learn lightgbm xgboost tqdm

import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import warnings
from datetime import datetime, timedelta
import gc

# Enable tqdm progress bars for pandas operations
tqdm.pandas()

# Suppress warnings
warnings.filterwarnings('ignore')

print("Starting data preparation process...")

# Load the datasets with progress feedback
print("Loading datasets...")
cadastro_pdv = pd.read_parquet('cadastro_pdv.parquet')
print(f"✓ Loaded PDV data: {cadastro_pdv.shape[0]} records")

cadastro_produtos = pd.read_parquet('cadastro_produtos.parquet')
print(f"✓ Loaded product data: {cadastro_produtos.shape[0]} records")

print("Loading transaction data (this might take a while)...")
dados_transacionais = pd.read_parquet('dados_transacionais.parquet')
print(f"✓ Loaded transaction data: {dados_transacionais.shape[0]} records")

# Step 1: Map transaction data to the correct PDV and product IDs
print("\nMapping transaction data to PDV and product IDs...")
# Rename columns to match the cadastro files
dados_transacionais = dados_transacionais.rename(columns={
    'internal_store_id': 'pdv',
    'internal_product_id': 'produto'
})

# Verify all transactions have matching PDVs and products
pdv_match = dados_transacionais['pdv'].isin(cadastro_pdv['pdv']).mean() * 100
produto_match = dados_transacionais['produto'].isin(cadastro_produtos['produto']).mean() * 100

print(f"PDV match rate: {pdv_match:.2f}%")
print(f"Product match rate: {produto_match:.2f}%")

# Keep only transactions with matching PDVs and products
dados_transacionais = dados_transacionais[
    dados_transacionais['pdv'].isin(cadastro_pdv['pdv']) &
    dados_transacionais['produto'].isin(cadastro_produtos['produto'])
]
print(f"✓ Filtered data: {dados_transacionais.shape[0]} valid transactions")

# Step 2: Handle outliers in quantity data
print("\nHandling outliers in quantity data...")
print(f"Before outlier removal: {dados_transacionais.shape[0]} records")
print(f"Quantity stats before: min={dados_transacionais['quantity'].min()}, max={dados_transacionais['quantity'].max()}")

# Calculate outlier thresholds using IQR method (per product)
def remove_outliers(group):
    q1 = group['quantity'].quantile(0.25)
    q3 = group['quantity'].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 3 * iqr  # More permissive lower bound to keep some negative values
    upper_bound = q3 + 5 * iqr  # More permissive upper bound

    return group[(group['quantity'] >= lower_bound) & (group['quantity'] <= upper_bound)]

# Apply outlier removal by product
print("Removing outliers (this may take a while)...")
dados_transacionais = dados_transacionais.groupby('produto').progress_apply(remove_outliers).reset_index(drop=True)

print(f"After outlier removal: {dados_transacionais.shape[0]} records")
print(f"Quantity stats after: min={dados_transacionais['quantity'].min()}, max={dados_transacionais['quantity'].max()}")

# Step 3: Filter to 2022 data only and create date features
print("\nFiltering to 2022 data and creating date features...")
dados_transacionais['transaction_date'] = pd.to_datetime(dados_transacionais['transaction_date'])
dados_2022 = dados_transacionais[dados_transacionais['transaction_date'].dt.year == 2022].copy()
print(f"✓ 2022 data: {dados_2022.shape[0]} transactions")

# Create date features
print("Creating date features...")
dados_2022['year'] = dados_2022['transaction_date'].dt.year
dados_2022['month'] = dados_2022['transaction_date'].dt.month
dados_2022['day'] = dados_2022['transaction_date'].dt.day
dados_2022['dayofweek'] = dados_2022['transaction_date'].dt.dayofweek
dados_2022['is_weekend'] = dados_2022['dayofweek'].isin([5, 6]).astype(int)

# Week calculations - making sure to use ISO calendar to get week numbers
dados_2022['week_number'] = dados_2022['transaction_date'].dt.isocalendar().week
# Get the start date of each week
dados_2022['week_start'] = dados_2022['transaction_date'].apply(
    lambda x: x - timedelta(days=x.weekday())
)

# Clear memory
gc.collect()

# Step 4: Aggregate data to weekly level
print("\nAggregating data to weekly level...")
# Define aggregation functions (using actual functions, not strings)
aggregation_dict = {
    'quantity': np.sum,
    'gross_value': np.sum,
    'net_value': np.sum,
    'gross_profit': np.sum,
    'discount': np.sum,
    'taxes': np.sum,
    'transaction_date': 'count'  # Count transactions per week/pdv/product
}

# Group by week, PDV, and product
weekly_agg = dados_2022.groupby(['week_number', 'week_start', 'pdv', 'produto']).agg(aggregation_dict)
weekly_agg = weekly_agg.reset_index()

# Rename transaction_date column to transaction_count
weekly_agg = weekly_agg.rename(columns={'transaction_date': 'transaction_count'})

print(f"✓ Created weekly aggregated data: {weekly_agg.shape[0]} records")

# Step 5: Create additional features
print("\nCreating additional features...")

# 5.1 Add PDV features
print("Adding PDV features...")
weekly_agg = weekly_agg.merge(cadastro_pdv, on='pdv', how='left')

# 5.2 Add product features
print("Adding product features...")
weekly_agg = weekly_agg.merge(
    cadastro_produtos[['produto', 'categoria', 'subcategoria', 'marca', 'fabricante']],
    on='produto', how='left'
)

# 5.3 Calculate rolling statistics (last 4 weeks)
print("Calculating rolling statistics (this may take a while)...")

# Sort by PDV, product, and week
weekly_agg = weekly_agg.sort_values(['pdv', 'produto', 'week_start'])

# Define a function to calculate rolling stats for a group
def calculate_rolling_stats(group):
    # Calculate rolling means with different windows
    group['qty_rolling_mean_4w'] = group['quantity'].rolling(window=4, min_periods=1).mean()
    group['qty_rolling_std_4w'] = group['quantity'].rolling(window=4, min_periods=1).std().fillna(0)

    # Calculate lag features
    group['qty_lag_1w'] = group['quantity'].shift(1)
    group['qty_lag_2w'] = group['quantity'].shift(2)
    group['qty_lag_3w'] = group['quantity'].shift(3)
    group['qty_lag_4w'] = group['quantity'].shift(4)

    # Calculate week-over-week changes
    group['qty_wow_change'] = group['quantity'] / group['qty_lag_1w'] - 1

    return group

# Apply rolling calculations to each PDV-product combination
# Using cuDF for potential GPU acceleration
import cudf
import cupy as cp

# Convert to cuDF DataFrame
weekly_agg_cudf = cudf.from_pandas(weekly_agg)

# Apply rolling calculations to each PDV-product combination
# Note: cuDF's groupby.apply is not as flexible as pandas, but we can achieve this
# using rolling and shift operations directly on the cuDF DataFrame after sorting.
weekly_agg_cudf = weekly_agg_cudf.sort_values(['pdv', 'produto', 'week_start'])

# Define rolling window
window_size = 4

# Calculate rolling means and stds
weekly_agg_cudf['qty_rolling_mean_4w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].rolling(window=window_size, min_periods=1).mean().reset_index(drop=True)
weekly_agg_cudf['qty_rolling_std_4w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].rolling(window=window_size, min_periods=1).std().fillna(0).reset_index(drop=True)

# Calculate lag features
weekly_agg_cudf['qty_lag_1w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].shift(1).fillna(0)
weekly_agg_cudf['qty_lag_2w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].shift(2).fillna(0)
weekly_agg_cudf['qty_lag_3w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].shift(3).fillna(0)
weekly_agg_cudf['qty_lag_4w'] = weekly_agg_cudf.groupby(['pdv', 'produto'])['quantity'].shift(4).fillna(0)

# Calculate week-over-week changes
# Need to handle division by zero if qty_lag_1w is 0
weekly_agg_cudf['qty_wow_change'] = (weekly_agg_cudf['quantity'] / weekly_agg_cudf['qty_lag_1w']) - 1
# Replace infinities and NaNs resulting from division by zero or NaNs in original data
weekly_agg_cudf['qty_wow_change'] = weekly_agg_cudf['qty_wow_change'].replace([cp.inf, -cp.inf], cp.nan).fillna(0)


# Convert back to pandas DataFrame
weekly_agg = weekly_agg_cudf.to_pandas()

# Fill NaN values
weekly_agg = weekly_agg.fillna({
    'qty_rolling_mean_4w': 0,
    'qty_rolling_std_4w': 0,
    'qty_lag_1w': 0,
    'qty_lag_2w': 0,
    'qty_lag_3w': 0,
    'qty_lag_4w': 0,
    'qty_wow_change': 0
})

# 5.4 Create seasonality indicators
print("Creating seasonality indicators...")
# Month of the year from week_start
weekly_agg['month'] = weekly_agg['week_start'].dt.month
# Quarter
weekly_agg['quarter'] = weekly_agg['week_start'].dt.quarter
# Is holiday season (weeks in December)
weekly_agg['is_holiday_season'] = (weekly_agg['month'] == 12).astype(int)

# 5.5 Create price features
print("Creating price features...")
# Calculate average unit price
weekly_agg['avg_unit_price'] = weekly_agg['gross_value'] / weekly_agg['quantity']
weekly_agg['avg_unit_price'] = weekly_agg['avg_unit_price'].replace([np.inf, -np.inf], np.nan).fillna(0)

# Calculate discount percentage
weekly_agg['discount_percentage'] = weekly_agg['discount'] / weekly_agg['gross_value']
weekly_agg['discount_percentage'] = weekly_agg['discount_percentage'].replace([np.inf, -np.inf], np.nan).fillna(0)

# 5.6 Create PDV-level aggregations
print("Creating PDV-level aggregations...")
pdv_weekly_agg = weekly_agg.groupby(['pdv', 'week_number']).agg({
    'quantity': 'sum',
    'gross_value': 'sum',
    'transaction_count': 'sum'
}).reset_index()

pdv_weekly_agg = pdv_weekly_agg.rename(columns={
    'quantity': 'pdv_total_quantity',
    'gross_value': 'pdv_total_gross_value',
    'transaction_count': 'pdv_total_transactions'
})

# Merge PDV-level aggregations back to main dataset
weekly_agg = weekly_agg.merge(
    pdv_weekly_agg,
    on=['pdv', 'week_number'],
    how='left'
)

# Calculate product's share of PDV's total sales
weekly_agg['product_quantity_share'] = weekly_agg['quantity'] / weekly_agg['pdv_total_quantity_y']
weekly_agg['product_value_share'] = weekly_agg['gross_value'] / weekly_agg['pdv_total_gross_value_y']

# Replace infinities and NaN values
for col in ['product_quantity_share', 'product_value_share']:
    weekly_agg[col] = weekly_agg[col].replace([np.inf, -np.inf], np.nan).fillna(0)

# 5.7 Create product-level aggregations
print("Creating product-level aggregations...")
product_weekly_agg = weekly_agg.groupby(['produto', 'week_number']).agg({
    'quantity': 'sum',
    'gross_value': 'sum'
}).reset_index()

product_weekly_agg = product_weekly_agg.rename(columns={
    'quantity': 'product_total_quantity',
    'gross_value': 'product_total_gross_value'
})

# Merge product-level aggregations back to main dataset
weekly_agg = weekly_agg.merge(
    product_weekly_agg,
    on=['produto', 'week_number'],
    how='left'
)

# Calculate PDV's share of product's total sales
weekly_agg['pdv_share_of_product'] = weekly_agg['quantity'] / weekly_agg['product_total_quantity']
weekly_agg['pdv_share_of_product'] = weekly_agg['pdv_share_of_product'].replace([np.inf, -np.inf], np.nan).fillna(0)

# Clean up memory
gc.collect()

# Step 6: Save the prepared dataset
print("\nSaving prepared dataset...")
weekly_agg.to_parquet('prepared_weekly_data.parquet', index=False)
print("✓ Saved prepared dataset to 'prepared_weekly_data.parquet'")

# Print final statistics
print("\nFinal dataset statistics:")
print(f"Number of records: {weekly_agg.shape[0]}")
print(f"Number of unique PDVs: {weekly_agg['pdv'].nunique()}")
print(f"Number of unique products: {weekly_agg['produto'].nunique()}")
print(f"Number of features: {weekly_agg.shape[1]}")
print("\nData preparation completed successfully!")